{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\"\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank you all so very much  \n",
      "\n",
      "the 0 6\n",
      "thank you to the academy  \n",
      "\n",
      "the 1 5\n",
      "thank you to all of you in this room  \n",
      "\n",
      "the 0 9\n",
      "i have to congratulate the other incredible nominees this year  \n",
      "\n",
      "the 1 10\n",
      "the revenant was the product of the tireless efforts of an unbelievable cast and crew  \n",
      "\n",
      "the 3 15\n",
      "first off to my brother in this endeavor mr tom hardy  \n",
      "\n",
      "the 0 11\n",
      "tom your talent on screen can only be surpassed by your friendship off screen thank you for creating a t ranscendent cinematic experience  \n",
      "\n",
      "the 0 23\n",
      "thank you to everybody at fox and new regency my entire team  \n",
      "\n",
      "the 0 12\n",
      "i have to thank everyone from the very onset of my career to my parents none of this would be possible without you  \n",
      "\n",
      "the 1 23\n",
      "and to my friends i love you dearly you know who you are  \n",
      "\n",
      "the 0 13\n",
      "and lastly i just want to say this making the revenant was about man s relationship to the natural world  \n",
      "\n",
      "the 2 20\n",
      "a world that we collectively felt in 2015 as the hottest year in recorded history  \n",
      "\n",
      "the 1 15\n",
      "our production needed to move to the southern tip of this planet just to be able to find snow  \n",
      "\n",
      "the 1 19\n",
      "climate change is real it is happening right now  \n",
      "\n",
      "the 0 9\n",
      "it is the most urgent threat facing our entire species and we need to work collectively together and stop procrastinating  \n",
      "\n",
      "the 1 20\n",
      "we need to support leaders around the world who do not speak for the big polluters but who speak for all of humanity for the indigenous people of the world for the billions and billions of underprivileged people out there who would be most affected by this  \n",
      "\n",
      "the 5 47\n",
      "for our children s children and for those people out there whose voices have been drowned out by the politics of greed  \n",
      "\n",
      "the 1 22\n",
      "i thank you all for this amazing award tonight  \n",
      "\n",
      "the 0 9\n",
      "let us not take this planet for granted  \n",
      "\n",
      "the 0 8\n",
      "i do not take tonight for granted  \n",
      "\n",
      "the 0 7\n",
      "thank you so very much  \n",
      "\n",
      "the 0 5\n"
     ]
    }
   ],
   "source": [
    "# Tokenize sentences\n",
    "dataset = nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W',' ',dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+',' ',dataset[i])\n",
    "\n",
    "\n",
    "# Creating word histogram\n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "            \n",
    "# Selecting best 100 features\n",
    "freq_words = heapq.nlargest(100,word2count,key=word2count.get)\n",
    "\n",
    "\n",
    "# IDF Dictionary\n",
    "word_idfs = {}\n",
    "for word in freq_words:\n",
    "    doc_count = 0\n",
    "    for data in dataset:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            doc_count += 1\n",
    "    word_idfs[word] = np.log(len(dataset)/(1+doc_count))\n",
    "    \n",
    "# TF Matrix\n",
    "tf_matrix = {}\n",
    "for word in freq_words:\n",
    "    doc_tf = []\n",
    "    for data in dataset:\n",
    "        frequency = 0\n",
    "        for w in nltk.word_tokenize(data):\n",
    "            if word == w:\n",
    "                frequency += 1\n",
    "        tf_word = frequency/len(nltk.word_tokenize(data))\n",
    "        print(data,'\\n')\n",
    "        print(word,frequency,len(nltk.word_tokenize(data)))\n",
    "        \n",
    "        doc_tf.append(tf_word)\n",
    "    tf_matrix[word] = doc_tf\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': [0.0,\n",
       "  0.2,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.2,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.043478260869565216,\n",
       "  0.0,\n",
       "  0.1,\n",
       "  0.06666666666666667,\n",
       "  0.05263157894736842,\n",
       "  0.0,\n",
       "  0.05,\n",
       "  0.10638297872340426,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Tf-Idf Model\n",
    "tfidf_matrix = []\n",
    "for word in tf_matrix.keys():\n",
    "    tfidf = []\n",
    "    for value in tf_matrix[word]:\n",
    "        score = value * word_idfs[word]\n",
    "        tfidf.append(score)\n",
    "    tfidf_matrix.append(tfidf)   \n",
    "    \n",
    "# Finishing the Tf-Tdf model\n",
    "X = np.asarray(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.12365622,  0.        ,  0.        ,\n",
       "        0.        ,  0.14121631,  0.        ,  0.        ,  0.        ,\n",
       "        0.23918075,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.27637135,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.32431836,\n",
       "        0.32431836,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
       "         3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "         6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  8,  8,\n",
       "         8,  8,  8,  8,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 12, 12,\n",
       "        12, 12, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 16, 17, 17, 17, 18,\n",
       "        18, 18, 19, 19, 19, 20, 20, 21, 21, 21, 22, 22, 23, 23, 24, 24, 25,\n",
       "        25, 26, 26, 27, 27, 28, 28, 29, 29, 30, 30, 31, 32, 33, 33, 34, 34,\n",
       "        35, 35, 36, 36, 37, 37, 38, 38, 39, 39, 40, 40, 41, 41, 42, 42, 43,\n",
       "        43, 44, 45, 46, 46, 47, 48, 48, 49, 49, 50, 50, 51, 52, 53, 54, 55,\n",
       "        56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "        73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "        90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([ 1,  3,  4,  8, 10, 11, 12, 14, 15, 16,  1,  2,  3,  5,  7,  8,  9,\n",
       "        10, 12, 14, 15,  0,  1,  2,  6,  7,  8,  9, 17, 20,  2,  4,  8, 12,\n",
       "        15, 16,  6, 15, 16, 17, 18, 19,  2,  3,  5,  8, 10, 12, 15, 17, 18,\n",
       "         0,  1,  2,  6,  7,  8, 17, 20,  4,  7,  9, 10, 14, 15, 16,  3,  8,\n",
       "         9, 10, 17, 19,  5,  7,  8,  9,  0,  2, 15, 17,  2,  5, 11,  6,  8,\n",
       "        12, 15,  9, 15, 10, 11, 15,  0,  8, 20,  3,  8, 16,  6, 15, 16, 11,\n",
       "        14, 15, 12, 14, 16, 13, 14, 15, 18, 19, 15, 16, 15, 16,  0, 20,  0,\n",
       "        20,  3, 11,  4, 10,  4, 10,  5,  6,  5,  6,  6,  6,  6, 11,  7, 14,\n",
       "         8, 15, 10, 12, 10, 16, 11, 14, 12, 18, 13, 14, 14, 15, 14, 15, 15,\n",
       "        19, 15, 15, 15, 16, 16, 17, 19, 18, 19, 18, 19,  1,  2,  3,  3,  3,\n",
       "         3,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  6,  6,  6,  6,\n",
       "         6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  9,  9,  9,  9,  9, 10]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(X>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset, columns = ['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank you all so very much \n",
      "thank you all so very much \n",
      "0 0 the\n",
      "thank you to the academy \n",
      "thank you to the academy \n",
      "1 1 the\n",
      "thank you to all of you in this room \n",
      "thank you to all of you in this room \n",
      "1 0 the\n",
      "i have to congratulate the other incredible nominees this year \n",
      "i have to congratulate the other incredible nominees this year \n",
      "2 1 the\n",
      "the revenant was the product of the tireless efforts of an unbelievable cast and crew \n",
      "the revenant was the product of the tireless efforts of an unbelievable cast and crew \n",
      "3 3 the\n",
      "first off to my brother in this endeavor mr tom hardy \n",
      "first off to my brother in this endeavor mr tom hardy \n",
      "3 0 the\n",
      "tom your talent on screen can only be surpassed by your friendship off screen thank you for creating a t ranscendent cinematic experience \n",
      "tom your talent on screen can only be surpassed by your friendship off screen thank you for creating a t ranscendent cinematic experience \n",
      "3 0 the\n",
      "thank you to everybody at fox and new regency my entire team \n",
      "thank you to everybody at fox and new regency my entire team \n",
      "3 0 the\n",
      "i have to thank everyone from the very onset of my career to my parents none of this would be possible without you \n",
      "i have to thank everyone from the very onset of my career to my parents none of this would be possible without you \n",
      "4 1 the\n",
      "and to my friends i love you dearly you know who you are \n",
      "and to my friends i love you dearly you know who you are \n",
      "4 0 the\n",
      "and lastly i just want to say this making the revenant was about man s relationship to the natural world \n",
      "and lastly i just want to say this making the revenant was about man s relationship to the natural world \n",
      "5 2 the\n",
      "a world that we collectively felt in 2015 as the hottest year in recorded history \n",
      "a world that we collectively felt in 2015 as the hottest year in recorded history \n",
      "6 1 the\n",
      "our production needed to move to the southern tip of this planet just to be able to find snow \n",
      "our production needed to move to the southern tip of this planet just to be able to find snow \n",
      "7 1 the\n",
      "climate change is real it is happening right now \n",
      "climate change is real it is happening right now \n",
      "7 0 the\n",
      "it is the most urgent threat facing our entire species and we need to work collectively together and stop procrastinating \n",
      "it is the most urgent threat facing our entire species and we need to work collectively together and stop procrastinating \n",
      "8 1 the\n",
      "we need to support leaders around the world who do not speak for the big polluters but who speak for all of humanity for the indigenous people of the world for the billions and billions of underprivileged people out there who would be most affected by this \n",
      "we need to support leaders around the world who do not speak for the big polluters but who speak for all of humanity for the indigenous people of the world for the billions and billions of underprivileged people out there who would be most affected by this \n",
      "9 5 the\n",
      "for our children s children and for those people out there whose voices have been drowned out by the politics of greed \n",
      "for our children s children and for those people out there whose voices have been drowned out by the politics of greed \n",
      "10 1 the\n",
      "i thank you all for this amazing award tonight \n",
      "i thank you all for this amazing award tonight \n",
      "10 0 the\n",
      "let us not take this planet for granted \n",
      "let us not take this planet for granted \n",
      "10 0 the\n",
      "i do not take tonight for granted \n",
      "i do not take tonight for granted \n",
      "10 0 the\n",
      "thank you so very much \n",
      "thank you so very much \n",
      "10 0 the\n"
     ]
    }
   ],
   "source": [
    "wordsss={}\n",
    "for word in freq_words:\n",
    "    #print(word)\n",
    "    toc_df = []\n",
    "    doc_count = 0\n",
    "    freq=0\n",
    "    for document in range(len(df)):\n",
    "        doc_1 = df.iloc[document,0]\n",
    "        tokens = nltk.word_tokenize(doc_1)\n",
    "        print(doc_1)\n",
    "#         print(len(doc_1))\n",
    "        if word in tokens:\n",
    "            doc_count+=1\n",
    "            freq+=1\n",
    "        freq_wordss = tokens.count(word)\n",
    "        print(doc_1)\n",
    "        print(freq,freq_wordss,word)\n",
    "            \n",
    "        toc_df.append(freq_wordss/len(nltk.word_tokenize(doc_1))*np.log(len(dataset)/(1+doc_count)))\n",
    "            \n",
    "    wordsss[word]=toc_df\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thank you all so very much '"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df['documents'][0].split(' ')).count('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1.split(' ').count('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordsss['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.043478260869565216,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.06666666666666667,\n",
       " 0.05263157894736842,\n",
       " 0.0,\n",
       " 0.05,\n",
       " 0.10638297872340426,\n",
       " 0.045454545454545456,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': [0.0,\n",
       "  0.47027505143269555,\n",
       "  0.0,\n",
       "  0.19459101490553132,\n",
       "  0.33164561532070652,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.062394979360405334,\n",
       "  0.0,\n",
       "  0.12527629684953681,\n",
       "  0.07324081924454065,\n",
       "  0.050793731370715106,\n",
       "  0.0,\n",
       "  0.042364893019360188,\n",
       "  0.078929504758444402,\n",
       "  0.029392143860229657,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_1.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank', 'you', 'so', 'very', 'much']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'to',\n",
       " 'you',\n",
       " 'of',\n",
       " 'for',\n",
       " 'this',\n",
       " 'thank',\n",
       " 'and',\n",
       " 'i',\n",
       " 'my',\n",
       " 'all',\n",
       " 'in',\n",
       " 'be',\n",
       " 'who',\n",
       " 'world',\n",
       " 'very',\n",
       " 'have',\n",
       " 'by',\n",
       " 'we',\n",
       " 'our',\n",
       " 'is',\n",
       " 'not',\n",
       " 'people',\n",
       " 'out',\n",
       " 'so',\n",
       " 'much',\n",
       " 'year',\n",
       " 'revenant',\n",
       " 'was',\n",
       " 'off',\n",
       " 'tom',\n",
       " 'your',\n",
       " 'screen',\n",
       " 'a',\n",
       " 'entire',\n",
       " 'would',\n",
       " 'just',\n",
       " 's',\n",
       " 'collectively',\n",
       " 'planet',\n",
       " 'it',\n",
       " 'most',\n",
       " 'need',\n",
       " 'do',\n",
       " 'speak',\n",
       " 'billions',\n",
       " 'there',\n",
       " 'children',\n",
       " 'tonight',\n",
       " 'take',\n",
       " 'granted',\n",
       " 'academy',\n",
       " 'room',\n",
       " 'congratulate',\n",
       " 'other',\n",
       " 'incredible',\n",
       " 'nominees',\n",
       " 'product',\n",
       " 'tireless',\n",
       " 'efforts',\n",
       " 'an',\n",
       " 'unbelievable',\n",
       " 'cast',\n",
       " 'crew',\n",
       " 'first',\n",
       " 'brother',\n",
       " 'endeavor',\n",
       " 'mr',\n",
       " 'hardy',\n",
       " 'talent',\n",
       " 'on',\n",
       " 'can',\n",
       " 'only',\n",
       " 'surpassed',\n",
       " 'friendship',\n",
       " 'creating',\n",
       " 't',\n",
       " 'ranscendent',\n",
       " 'cinematic',\n",
       " 'experience',\n",
       " 'everybody',\n",
       " 'at',\n",
       " 'fox',\n",
       " 'new',\n",
       " 'regency',\n",
       " 'team',\n",
       " 'everyone',\n",
       " 'from',\n",
       " 'onset',\n",
       " 'career',\n",
       " 'parents',\n",
       " 'none',\n",
       " 'possible',\n",
       " 'without',\n",
       " 'friends',\n",
       " 'love',\n",
       " 'dearly',\n",
       " 'know',\n",
       " 'are',\n",
       " 'lastly']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1.split(' ').count('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1.split().count('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank', 'you', 'so', 'very', 'much']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['documents'][2].split().count('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'thank' in doc_1:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
